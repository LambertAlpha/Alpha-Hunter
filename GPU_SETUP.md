# Mac Mini GPU é…ç½®æŒ‡å—

## ğŸ Apple Silicon GPU æ”¯æŒ

ä½ çš„ Mac Mini ä½¿ç”¨ **Apple Silicon (M1/M2/M3)** èŠ¯ç‰‡ï¼Œæ”¯æŒé€šè¿‡ **MPS (Metal Performance Shaders)** åŠ é€Ÿæ·±åº¦å­¦ä¹ ï¼

### ç¡¬ä»¶æ£€æŸ¥

```bash
# æŸ¥çœ‹Macå‹å·å’ŒèŠ¯ç‰‡
system_profiler SPHardwareDataType | grep "Chip\|Model"
```

å¦‚æœæ˜¾ç¤º M1/M2/M3ï¼Œè¯´æ˜æ”¯æŒ GPU åŠ é€Ÿï¼

---

## ğŸš€ ç¯å¢ƒé…ç½®

### æ–¹æ³• 1: è‡ªåŠ¨å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/lambertlin/Projects/Alpha-Hunter

# è¿è¡Œå®‰è£…è„šæœ¬
bash setup_environment.sh
```

### æ–¹æ³• 2: æ‰‹åŠ¨å®‰è£…

```bash
# 1. åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml

# 2. æ¿€æ´»ç¯å¢ƒ
conda activate ml

# 3. å®‰è£…PyTorch (è‡ªåŠ¨æ”¯æŒMPS)
pip install torch torchvision torchaudio

# 4. éªŒè¯GPU
python -c "import torch; print('MPSå¯ç”¨:', torch.backends.mps.is_available())"
```

---

## ğŸ¯ è®­ç»ƒæ—¶ä½¿ç”¨GPU

### TFAè®­ç»ƒ

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨MPS
python train_tfa.py --device auto --epochs 50

# æˆ–æ˜¾å¼æŒ‡å®šMPS
python train_tfa.py --device mps --epochs 50
```

### Baselineè®­ç»ƒ

```bash
python train.py --model transformer --device auto
```

### Jupyter Notebook

```python
# åœ¨notebookä¸­æ£€æŸ¥GPU
import torch

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")
print(f"MPSå·²æ„å»º: {torch.backends.mps.is_built()}")

# åˆ›å»ºtensoræµ‹è¯•
if torch.backends.mps.is_available():
    device = torch.device("mps")
    x = torch.ones(5, device=device)
    print(f"âœ… GPUæµ‹è¯•æˆåŠŸ: {x.device}")
```

---

## âš¡ æ€§èƒ½å¯¹æ¯”

| ç¡¬ä»¶ | è®­ç»ƒé€Ÿåº¦ | æ¨èbatch_size |
|------|----------|----------------|
| **MPS (M1/M2/M3)** | **å¿« 3-5å€** | **128-256** |
| CPU | æ…¢ | 64-128 |

**å»ºè®®**ï¼š
- TFAè®­ç»ƒï¼šä½¿ç”¨ `--device mps --batch_size 256`
- å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°‘ `--batch_size` æˆ– `--d_model`

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: "MPS backend not available"

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é‡æ–°å®‰è£…PyTorch
pip uninstall torch torchvision torchaudio
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu
```

### Q2: å†…å­˜æº¢å‡º (OOM)

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°batch_size
python train_tfa.py --batch_size 64 --device mps

# æˆ–å‡å°æ¨¡å‹
python train_tfa.py --d_model 64 --device mps
```

### Q3: è®­ç»ƒæ—¶å¡ä½

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æŸäº›æ“ä½œå¯èƒ½ä¸æ”¯æŒMPSï¼Œè‡ªåŠ¨å›é€€åˆ°CPU
export PYTORCH_ENABLE_MPS_FALLBACK=1
python train_tfa.py --device mps
```

### Q4: æƒ³å¼ºåˆ¶ä½¿ç”¨CPU

```bash
python train_tfa.py --device cpu
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æ´»åŠ¨ç›‘è§†å™¨

```
Spotlightæœç´¢: æ´»åŠ¨ç›‘è§†å™¨
â†’ çª—å£ â†’ GPUå†å²è®°å½•
```

è®­ç»ƒæ—¶åº”è¯¥çœ‹åˆ°GPUä½¿ç”¨ç‡ä¸Šå‡ï¼

### å‘½ä»¤è¡Œç›‘æ§

```bash
# å®‰è£…ç›‘æ§å·¥å…·
pip install asitop

# è¿è¡Œç›‘æ§
sudo asitop
```

---

## âœ… éªŒè¯å®‰è£…

è¿è¡Œè¿™ä¸ªè„šæœ¬éªŒè¯ä¸€åˆ‡æ­£å¸¸ï¼š

```python
# test_gpu.py
import torch
import sys

print("="*60)
print("Alpha-Hunter GPU ç¯å¢ƒæµ‹è¯•")
print("="*60)
print(f"Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")
print(f"MPSå·²æ„å»º: {torch.backends.mps.is_built()}")

if torch.backends.mps.is_available():
    print("\nâœ… GPUåŠ é€Ÿå·²å¯ç”¨!")
    print("   å»ºè®®ä½¿ç”¨: --device mps æˆ– --device auto")
    
    # é€Ÿåº¦æµ‹è¯•
    print("\nğŸƒ é€Ÿåº¦æµ‹è¯•...")
    import time
    
    size = 1000
    x = torch.randn(size, size)
    
    # CPU
    start = time.time()
    y = torch.matmul(x, x)
    cpu_time = time.time() - start
    
    # MPS
    x_mps = x.to('mps')
    start = time.time()
    y_mps = torch.matmul(x_mps, x_mps)
    torch.mps.synchronize()
    mps_time = time.time() - start
    
    print(f"   CPUæ—¶é—´: {cpu_time:.4f}s")
    print(f"   MPSæ—¶é—´: {mps_time:.4f}s")
    print(f"   åŠ é€Ÿ: {cpu_time/mps_time:.2f}x")
else:
    print("\nâš ï¸  MPSä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    print("   è¿™ä¸å½±å“åŠŸèƒ½ï¼Œåªæ˜¯é€Ÿåº¦è¾ƒæ…¢")

print("\n" + "="*60)
```

è¿è¡Œï¼š
```bash
python test_gpu.py
```

---

## ğŸ“ æ¨èé…ç½®

### æ—¥å¸¸å¼€å‘
```bash
# ä½¿ç”¨Jupyterï¼Œè‡ªåŠ¨GPUåŠ é€Ÿ
conda activate ml
jupyter notebook notebooks/TFA_Demo.ipynb
```

### å®Œæ•´è®­ç»ƒ
```bash
conda activate ml
python train_tfa.py --device auto --epochs 50 --batch_size 256 --analyze
```

### è¶…å‚æ•°æœç´¢
```bash
# å¯ä»¥å¹¶è¡Œè¿è¡Œå¤šä¸ªå®éªŒ
python train_tfa.py --device mps --d_model 64 &
python train_tfa.py --device mps --d_model 128 &
python train_tfa.py --device mps --d_model 256 &
```

---

**é¢„æœŸè®­ç»ƒæ—¶é—´**ï¼ˆMac Mini M1/M2ï¼‰ï¼š
- TFA Demo (å°æ ·æœ¬): ~2-3åˆ†é’Ÿ
- TFAå®Œæ•´è®­ç»ƒ: ~20-40åˆ†é’Ÿï¼ˆvs CPU: 1-2å°æ—¶ï¼‰
- æ‰€æœ‰æ¨¡å‹å¯¹æ¯”: ~1å°æ—¶ï¼ˆvs CPU: 3-4å°æ—¶ï¼‰

ğŸš€ äº«å—GPUåŠ é€Ÿçš„å¿«æ„Ÿï¼

