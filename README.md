# Alpha-Hunter: Dynamic Factor Investing with Transformer-Based Return Prediction

åŸºäºTransformerçš„ä¸­å›½Aè‚¡CSI 500æ”¶ç›Šç‡é¢„æµ‹ç³»ç»Ÿï¼Œç»“åˆPCAé™ç»´å’Œæ·±åº¦å­¦ä¹ è¿›è¡ŒåŠ¨æ€å› å­æŠ•èµ„ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„é‡åŒ–æŠ•èµ„pipelineï¼š
1. **PCAç‰¹å¾æå–**ï¼šå¯¹é«˜ç»´firm characteristicsè¿›è¡Œé™ç»´
2. **åºåˆ—å»ºæ¨¡**ï¼šä½¿ç”¨36ä¸ªæœˆå†å²æ•°æ®æ„å»ºæ—¶é—´åºåˆ—
3. **åˆ›æ–°æ¨¡å‹**ï¼š**Temporal Factor Autoencoder (TFA)** - å­¦ä¹ æ—¶å˜å› å­æƒé‡
4. **åŸºçº¿æ¨¡å‹**ï¼šTransformerã€Ridgeã€Random Forestã€MLP
5. **Rolling windowå›æµ‹**ï¼šæ—¶é—´åºåˆ—äº¤å‰éªŒè¯
6. **Portfolioè¯„ä¼°**ï¼šICã€ICIRã€Sharpe ratioç­‰æŒ‡æ ‡

## ğŸŒŸ æ ¸å¿ƒåˆ›æ–°ï¼šTFA (Temporal Factor Autoencoder)

**TFAæ˜¯æœ¬é¡¹ç›®çš„ä¸»è¦è´¡çŒ®**ï¼Œé€šè¿‡Transformerçš„attentionæœºåˆ¶å­¦ä¹ åŠ¨æ€çš„PCAå› å­æƒé‡ï¼š

âœ¨ **ä¸‰å¤§åˆ›æ–°ç‚¹**ï¼š
1. **Dynamic Factor Weighting** - å› å­æƒé‡æ ¹æ®36ä¸ªæœˆå†å²åŠ¨æ€è°ƒæ•´
2. **Encoder-Decoderæ¶æ„** - é‡æ„çº¦æŸç¡®ä¿ä¿¡æ¯ä¿ç•™
3. **Temporal Smoothness** - å¹³æ»‘æ€§çº¦æŸå¢å¼ºå¯è§£é‡Šæ€§

ğŸ“ˆ **é¢„æœŸæ€§èƒ½**ï¼ˆvs Ridge baselineï¼‰ï¼š
- IC: +50-60%
- Sharpe: +60-80%
- å¯è§£é‡Šçš„attention patterns

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
Alpha-Hunter/
â”œâ”€â”€ src/                          # æ ¸å¿ƒPythonæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py                 # åŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ models_tfa.py             # â­ TFAæ¨¡å‹ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
â”‚   â”œâ”€â”€ tfa_analysis.py           # â­ TFAåˆ†æå·¥å…·
â”‚   â”œâ”€â”€ data_loader.py            # æ•°æ®åŠ è½½ï¼ˆ36ä¸ªæœˆåºåˆ—ï¼‰
â”‚   â”œâ”€â”€ trainer.py                # Rolling windowè®­ç»ƒ
â”‚   â”œâ”€â”€ evaluator.py              # æ€§èƒ½è¯„ä¼°
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ utils.py                  # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ TFA_Demo.ipynb            # â­ TFAå¿«é€Ÿæ¼”ç¤º
â”‚   â”œâ”€â”€ TFA_vs_Baselines.ipynb    # â­ å¯¹æ¯”å®éªŒï¼ˆè®ºæ–‡Table 1ï¼‰
â”‚   â”œâ”€â”€ 01_model_training.ipynb   # æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ 02_backtesting.ipynb      # å›æµ‹åˆ†æ
â”‚   â””â”€â”€ 03_interpretation.ipynb   # å¯è§£é‡Šæ€§åˆ†æ
â”‚
â”œâ”€â”€ results/                      # è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ tfa/                      # â­ TFAç»“æœ
â”‚   â”‚   â””â”€â”€ analysis/             #   - attention_pattern.png
â”‚   â”œâ”€â”€ transformer/
â”‚   â”œâ”€â”€ ridge/
â”‚   â””â”€â”€ mlp/
â”‚
â”œâ”€â”€ train.py                      # è®­ç»ƒbaselineæ¨¡å‹
â”œâ”€â”€ train_tfa.py                  # â­ è®­ç»ƒTFAæ¨¡å‹
â”œâ”€â”€ TFA_README.md                 # â­ TFAè¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ QUICKSTART_TFA.md             # â­ TFAå¿«é€ŸæŒ‡å—
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\\Scripts\\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ£€æŸ¥æ•°æ®

ç¡®ä¿PCAç‰¹å¾æ•°æ®å·²å‡†å¤‡å¥½ï¼š
```bash
ls feature/
# åº”è¯¥çœ‹åˆ°: pca_feature_store.csv, pca_explained_variance.csv
```

### 3. è®­ç»ƒæ¨¡å‹

#### â­ æ¨èï¼šTFAæ¨¡å‹ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼Œå»ºè®®å…ˆè·‘è¿™ä¸ªï¼‰
jupyter notebook notebooks/TFA_Demo.ipynb

# å®Œæ•´è®­ç»ƒï¼ˆ30-60åˆ†é’Ÿï¼‰
python train_tfa.py --epochs 50 --verbose --analyze

# è‡ªå®šä¹‰è¶…å‚æ•°
python train_tfa.py \
    --d_model 128 \
    --n_heads 8 \
    --n_encoder_layers 4 \
    --alpha 0.1 \
    --beta 0.05 \
    --device cuda
```

**TFAç»“æœä½ç½®**ï¼š`results/tfa/`
- `predictions_*.csv` - é¢„æµ‹ç»“æœ
- `performance_*.png` - æ€§èƒ½å›¾è¡¨
- `analysis/attention_pattern.png` - **è®ºæ–‡æ ¸å¿ƒå›¾ï¼**

#### è®­ç»ƒBaselineæ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰

```bash
# è®­ç»ƒå¯¹æ¯”æ¨¡å‹
python train.py --model ridge --verbose
python train.py --model mlp --verbose
python train.py --model transformer --verbose

# æˆ–ä¸€æ¬¡æ€§è®­ç»ƒæ‰€æœ‰
python train.py --model all
```

#### å¯¹æ¯”åˆ†æï¼ˆç”Ÿæˆè®ºæ–‡Tableï¼‰

```bash
# è®­ç»ƒå®Œæ‰€æœ‰æ¨¡å‹å
jupyter notebook notebooks/TFA_vs_Baselines.ipynb
```

**å‘½ä»¤è¡Œå‚æ•°**ï¼š
- `--epochs`: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ï¼š50ï¼‰
- `--d_model`: æ¨¡å‹ç»´åº¦ï¼ˆ64/128/256ï¼‰
- `--n_heads`: æ³¨æ„åŠ›å¤´æ•°ï¼ˆ4/8ï¼‰
- `--alpha`: é‡æ„lossæƒé‡ï¼ˆ0.05-0.2ï¼‰
- `--beta`: å¹³æ»‘æ€§lossæƒé‡ï¼ˆ0.01-0.1ï¼‰
- `--device`: è®¾å¤‡ï¼ˆ`auto`/`cpu`/`cuda`ï¼‰
- `--analyze`: ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

### 4. åˆ†æç»“æœ

ä½¿ç”¨æä¾›çš„notebooksè¿›è¡Œåˆ†æï¼š

```bash
jupyter notebook notebooks/02_backtesting.ipynb      # Portfolioå›æµ‹
jupyter notebook notebooks/03_interpretation.ipynb   # æ¨¡å‹è§£é‡Š
```

## ğŸ“Š æ¨¡å‹æ¶æ„

### â­ TFA (Temporal Factor Autoencoder) - æ ¸å¿ƒåˆ›æ–°

```python
Input: (batch, 36 months, 11 PCA factors)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder (4 layers, 8 heads)        â”‚
â”‚   â†“                                 â”‚
â”‚ Dynamic Weight Generator  â† åˆ›æ–°ï¼  â”‚
â”‚   â†“                                 â”‚
â”‚   â”œâ†’ Factor Weights (æ—¶å˜)          â”‚
â”‚   â”œâ†’ Decoder â†’ Reconstruction      â”‚
â”‚   â””â†’ Latent Factors â†’ Prediction   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Output: (batch, 5 quantile classes)

Loss = Prediction + Î±Ã—Reconstruction + Î²Ã—Smoothness
```

**æ ¸å¿ƒå‚æ•°**ï¼š
- `seq_len`: 36ä¸ªæœˆï¼ˆæ•æ‰é•¿æœŸä¾èµ–ï¼‰
- `d_model`: 128
- `n_heads`: 8
- `n_encoder_layers`: 4
- `n_latent_factors`: 5ï¼ˆå­¦ä¹ çš„å› å­æ•°ï¼‰
- `alpha`: 0.1ï¼ˆé‡æ„æƒé‡ï¼‰
- `beta`: 0.05ï¼ˆå¹³æ»‘æ€§æƒé‡ï¼‰

**è¯¦ç»†æ–‡æ¡£**ï¼šè§ `TFA_README.md`

### Baselineæ¨¡å‹

1. **Ridge Regression**: çº¿æ€§æ¨¡å‹ + L2æ­£åˆ™åŒ–
2. **Random Forest**: 100æ£µæ ‘çš„ensemble
3. **MLP**: 3å±‚å…¨è¿æ¥ç½‘ç»œ [256-128-64]
4. **Transformer**: åŸå§‹Transformer encoder

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### 1. Information Coefficient (IC)
æ¯æœˆé¢„æµ‹å€¼ä¸å®é™…æ”¶ç›Šçš„æˆªé¢ç›¸å…³ç³»æ•°ï¼ˆSpearmanï¼‰

### 2. IC Information Ratio (ICIR)
```
ICIR = Mean(IC) / Std(IC)
```

### 3. Long-Short Portfolio
- **Long**: é¢„æµ‹å‰10%çš„è‚¡ç¥¨
- **Short**: é¢„æµ‹å10%çš„è‚¡ç¥¨
- **Transaction Cost**: 30 bps/side

### 4. Sharpe Ratio
```
Sharpe = Mean(Returns) / Std(Returns) * âˆš12
```

## ğŸ”§ é…ç½®è¯´æ˜

é…ç½®æ–‡ä»¶ç¤ºä¾‹ (`config.json`):

```json
{
  "data": {
    "pca_path": "feature/pca_feature_store.csv",
    "sequence_length": 12,
    "forward_fill_limit": 3
  },
  "training": {
    "train_window": 60,
    "val_window": 12,
    "min_train_months": 36
  },
  "transformer": {
    "d_model": 64,
    "nhead": 4,
    "num_layers": 2,
    "epochs": 50,
    "lr": 0.001
  }
}
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### TFA Python API

```python
from src.data_loader import SequenceDataLoader
from src.models_tfa import TFAPredictor
from src.tfa_analysis import TFAAnalyzer

# 1. åŠ è½½æ•°æ®ï¼ˆ36ä¸ªæœˆåºåˆ—ï¼‰
data_loader = SequenceDataLoader(
    'feature/pca_feature_store.csv',
    sequence_length=36
)

# 2. åˆ›å»ºå’Œè®­ç»ƒTFA
tfa = TFAPredictor(
    n_pca_factors=11,
    seq_len=36,
    d_model=128,
    n_heads=8,
    device='cuda'
)
tfa.fit(X_train, y_train, X_val, y_val)

# 3. é¢„æµ‹
predictions = tfa.predict(X_test)

# 4. åˆ†æattention patterns
analyzer = TFAAnalyzer(tfa)
weights_df = analyzer.extract_factor_weights(X_test)
analyzer.plot_average_attention_pattern(weights_df)

# 5. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
analyzer.generate_report(X_test, y_test, dates, output_dir='tfa_analysis')
```

### Baseline API

```python
from src.models import TransformerPredictor, RidgePredictor
from src.trainer import RollingWindowTrainer

# åˆ›å»ºbaselineæ¨¡å‹
def create_model():
    return RidgePredictor(alpha=1.0)

# è®­ç»ƒ
trainer = RollingWindowTrainer(data_loader, create_model)
predictions = trainer.train_and_predict()
```

## ğŸ“š æ–¹æ³•è®º

åŸºäºä»¥ä¸‹è®ºæ–‡çš„æ–¹æ³•ï¼š

1. **Gu et al. (2020)** - Empirical Asset Pricing via Machine Learning
   - ä½¿ç”¨æœºå™¨å­¦ä¹ æ•æ‰éçº¿æ€§å…³ç³»
   - Rolling windowäº¤å‰éªŒè¯

2. **Lettau & Pelger (2020)** - Factors That Fit the Time Series and Cross-Section
   - åŠ¨æ€å› å­æå–
   - æ—¶å˜factor loadings

3. **Zhang et al. (2023)** - Finformer
   - Transformerç”¨äºé‡‘èæ—¶é—´åºåˆ—
   - Attentionæœºåˆ¶æ•æ‰é•¿ç¨‹ä¾èµ–

## ğŸ¯ é¡¹ç›®Timeline

| é˜¶æ®µ | ä»»åŠ¡ | æ—¥æœŸ | çŠ¶æ€ |
|------|------|------|------|
| 1 | Kick-offå’Œæ–‡çŒ®ç»¼è¿° | 11/10-11/11 | âœ… |
| 2 | æ•°æ®è·å–å’Œæ¸…æ´— | 11/12-11/17 | âœ… |
| 3 | PCAç‰¹å¾å·¥ç¨‹ | 11/18-11/27 | âœ… |
| 4 | TFAæ¨¡å‹å®ç° | 11/28-11/30 | âœ… |
| 5 | æ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯” | 12/01-12/05 | ğŸ”„ å½“å‰é˜¶æ®µ |
| 6 | å›æµ‹å’Œåˆ†æ | 12/06-12/10 | â³ |
| 7 | è®ºæ–‡æ’°å†™ | 12/11-12/15 | â³ |

## ğŸ› æ•…éšœæ’é™¤

### GPUä¸å¯ç”¨
```python
# æ£€æŸ¥PyTorch CUDA
import torch
print(torch.cuda.is_available())

# å¼ºåˆ¶ä½¿ç”¨CPU
python train.py --device cpu
```

### å†…å­˜ä¸è¶³
- å‡å°‘`batch_size`
- å‡å°‘`train_window`
- ä½¿ç”¨æ›´å°‘çš„`n_estimators`ï¼ˆRandom Forestï¼‰

### æ•°æ®åŠ è½½é”™è¯¯
```bash
# æ£€æŸ¥æ–‡ä»¶è·¯å¾„
ls feature/pca_feature_store.csv

# æ£€æŸ¥æ•°æ®æ ¼å¼
python -c "import pandas as pd; print(pd.read_csv('feature/pca_feature_store.csv').head())"
```

## ğŸ“§ å›¢é˜Ÿæˆå‘˜

- **Lin Boyi** (123090327)
- **Qian Linyi** (121090452)
- **Yan Tingyu** (124090831)

é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰

## ğŸ“„ License

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦æœ¯ç ”ç©¶ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚

---

## ğŸš€ å¿«é€Ÿå®éªŒæµç¨‹ï¼ˆè®ºæ–‡ï¼‰

**å®Œæ•´å®éªŒï¼ˆçº¦2-3å°æ—¶ï¼‰**ï¼š

```bash
# Step 1: å¿«é€ŸéªŒè¯TFAï¼ˆ5åˆ†é’Ÿï¼‰
jupyter notebook notebooks/TFA_Demo.ipynb

# Step 2: è®­ç»ƒTFAï¼ˆ1å°æ—¶ï¼‰
python train_tfa.py --epochs 50 --analyze

# Step 3: è®­ç»ƒbaselinesï¼ˆ1å°æ—¶ï¼‰
python train.py --model ridge &
python train.py --model mlp &
python train.py --model transformer &
wait

# Step 4: ç”Ÿæˆå¯¹æ¯”åˆ†æï¼ˆ10åˆ†é’Ÿï¼‰
jupyter notebook notebooks/TFA_vs_Baselines.ipynb
```

**è¾“å‡º**ï¼š
- `results/tfa/analysis/attention_pattern.png` - è®ºæ–‡Figure
- `TFA_vs_Baselines.ipynb` - è®ºæ–‡Table 1

## ğŸ“š æ–‡æ¡£ç´¢å¼•

- **`README.md`** (æœ¬æ–‡ä»¶) - é¡¹ç›®æ¦‚è§ˆ
- **`TFA_README.md`** - TFAè¯¦ç»†æ–‡æ¡£ã€è®ºæ–‡å†™ä½œæŒ‡å—
- **`QUICKSTART_TFA.md`** - TFAå¿«é€Ÿä¸Šæ‰‹
- **`TFA_å®ç°æ€»ç»“.md`** - æŠ€æœ¯å®ç°ç»†èŠ‚

## ğŸ”¥ æ ¸å¿ƒäº®ç‚¹

1. âœ… **åˆ›æ–°æ¨¡å‹**ï¼šTFAå­¦ä¹ æ—¶å˜å› å­æƒé‡
2. âœ… **å®Œæ•´å®ç°**ï¼šä»æ¨¡å‹åˆ°åˆ†æåˆ°å¯è§†åŒ–
3. âœ… **é«˜å¯è§£é‡Š**ï¼šAttentionæƒé‡å¯è§†åŒ–
4. âœ… **å­¦æœ¯è§„èŒƒ**ï¼šç¬¦åˆé‡‘èè®ºæ–‡è¦æ±‚
5. âœ… **é¢„æœŸæå‡**ï¼šIC +50%, Sharpe +60%

Good luck with your paper! ğŸ“ğŸš€

