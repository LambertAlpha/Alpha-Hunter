{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Temporal Factor Autoencoder (TFA) - Demo\n",
        "\n",
        "**æ ¸å¿ƒåˆ›æ–°ï¼šç”¨Transformerå­¦ä¹ æ—¶å˜å› å­æƒé‡**\n",
        "\n",
        "æœ¬notebookå±•ç¤ºTFAçš„æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
        "1. åŠ è½½å’Œå‡†å¤‡æ•°æ®ï¼ˆ36ä¸ªæœˆåºåˆ—ï¼‰\n",
        "2. è®­ç»ƒTFAæ¨¡å‹\n",
        "3. å¯è§†åŒ–åŠ¨æ€å› å­æƒé‡\n",
        "4. åˆ†æå­¦ä¹ åˆ°çš„latent factors\n",
        "5. ç”Ÿæˆäº¤æ˜“ä¿¡å·\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data_loader import SequenceDataLoader\n",
        "from src.models_tfa import TFAPredictor, TemporalFactorAutoencoder\n",
        "from src.tfa_analysis import TFAAnalyzer\n",
        "from src.config import Config\n",
        "from src.utils import set_random_seed, check_gpu_availability\n",
        "\n",
        "sns.set_theme(style='whitegrid')\n",
        "pd.set_option('display.max_columns', 20)\n",
        "set_random_seed(42)\n",
        "\n",
        "print(\"âœ… Libraries loaded successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {check_gpu_availability()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. åŠ è½½æ•°æ®\n",
        "\n",
        "**é‡è¦å˜åŒ–**: åºåˆ—é•¿åº¦ä»12ä¸ªæœˆæ”¹ä¸º**36ä¸ªæœˆ**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½é…ç½®\n",
        "config = Config()\n",
        "device = check_gpu_availability()\n",
        "\n",
        "# åŠ è½½PCAæ•°æ®ï¼ˆ36ä¸ªæœˆåºåˆ—ï¼‰\n",
        "PCA_PATH = '../feature/pca_feature_store.csv'\n",
        "data_loader = SequenceDataLoader(\n",
        "    pca_path=PCA_PATH,\n",
        "    sequence_length=36,  # â† æ”¹æˆ36ä¸ªæœˆï¼\n",
        "    forward_fill_limit=3\n",
        ")\n",
        "\n",
        "# æ•°æ®ç»Ÿè®¡\n",
        "stats = data_loader.get_statistics()\n",
        "print(f\"\\\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"  Dates: {stats['n_dates']}\")\n",
        "print(f\"  Assets: {stats['n_assets']}\")\n",
        "print(f\"  PCA Features: {stats['n_features']}\")\n",
        "print(f\"  Date Range: {stats['date_range'][0].date()} to {stats['date_range'][1].date()}\")\n",
        "print(f\"  Avg Assets/Date: {stats['avg_assets_per_date']:.1f}\")\n",
        "\n",
        "data_loader.df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. åˆå§‹åŒ–TFAæ¨¡å‹\n",
        "\n",
        "å±•ç¤ºæ¨¡å‹æ¶æ„å’Œå‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºTFAæ¨¡å‹\n",
        "tfa = TFAPredictor(\n",
        "    n_pca_factors=stats['n_features'],  # 11\n",
        "    seq_len=36,                         # 3å¹´å†å²\n",
        "    d_model=128,                        # æ¨¡å‹ç»´åº¦\n",
        "    n_heads=8,                          # 8ä¸ªæ³¨æ„åŠ›å¤´\n",
        "    n_encoder_layers=4,                 # 4å±‚encoder\n",
        "    n_decoder_layers=2,                 # 2å±‚decoder\n",
        "    n_latent_factors=5,                 # å­¦ä¹ 5ä¸ªlatent factors\n",
        "    dropout=0.1,\n",
        "    n_classes=5,                        # 5-åˆ†ä½æ•°åˆ†ç±»\n",
        "    lr=1e-3,\n",
        "    epochs=20,  # å‡å°‘epochsç”¨äºå¿«é€Ÿæ¼”ç¤º\n",
        "    batch_size=128,\n",
        "    alpha=0.1,   # é‡æ„lossæƒé‡\n",
        "    beta=0.05,   # å¹³æ»‘æ€§lossæƒé‡\n",
        "    gamma=0.01,  # æ­£äº¤æ€§lossæƒé‡\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"\\\\nğŸ¤– TFA Model Initialized:\")\n",
        "print(f\"  Total Parameters: {tfa.model.count_parameters():,}\")\n",
        "print(f\"  Input: (batch, 36 months, 11 PCA factors)\")\n",
        "print(f\"  Output: (batch, 5 quantile classes)\")\n",
        "print(f\"\\\\n  Encoder: 4 layers Ã— 8 heads\")\n",
        "print(f\"  Decoder: 2 layers Ã— 8 heads\")\n",
        "print(f\"  Latent Factors: 5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå°æ ·æœ¬æ¼”ç¤ºï¼‰\n",
        "\n",
        "ä¸ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œæˆ‘ä»¬åªç”¨ä¸€å°éƒ¨åˆ†æ•°æ®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é€‰æ‹©ä¸€ä¸ªæµ‹è¯•æ—¥æœŸ\n",
        "test_dates = data_loader.dates[-20:]  # æœ€è¿‘20ä¸ªæœˆ\n",
        "\n",
        "# æ„å»ºè®­ç»ƒé›†ï¼ˆç”¨å‰15ä¸ªæœˆï¼‰\n",
        "X_train_list = []\n",
        "y_train_list = []\n",
        "\n",
        "for date in test_dates[:15]:\n",
        "    try:\n",
        "        data_dict = data_loader.build_sequences(date, include_target=True, return_dict=True)\n",
        "        X_train_list.append(data_dict['X'])\n",
        "        y_train_list.append(data_dict['y'])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "X_train = np.vstack(X_train_list)\n",
        "y_train = np.concatenate(y_train_list)\n",
        "\n",
        "print(f\"\\\\nğŸ“¦ Training Data:\")\n",
        "print(f\"  X_train shape: {X_train.shape}  # (samples, 36 months, 11 features)\")\n",
        "print(f\"  y_train shape: {y_train.shape}  # (samples,)\")\n",
        "print(f\"  Returns range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. è®­ç»ƒTFAæ¨¡å‹\n",
        "\n",
        "è§‚å¯Ÿå¤šä»»åŠ¡lossçš„å˜åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒæ¨¡å‹\n",
        "print(\"ğŸš€ Training TFA...\\\\n\")\n",
        "\n",
        "tfa.fit(X_train, y_train, verbose=True)\n",
        "\n",
        "print(\"\\\\nâœ… Training completed!\")\n",
        "\n",
        "# å¯è§†åŒ–è®­ç»ƒå†å²\n",
        "if len(tfa.training_history) > 0:\n",
        "    history_df = pd.DataFrame(tfa.training_history)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "    \n",
        "    # Total loss\n",
        "    axes[0].plot(history_df['epoch'], history_df['train_loss'], linewidth=2)\n",
        "    axes[0].set_title('Total Loss', fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Component losses\n",
        "    axes[1].plot(history_df['epoch'], history_df['train_prediction'], label='Prediction', linewidth=2)\n",
        "    axes[1].plot(history_df['epoch'], history_df['train_reconstruction'], label='Reconstruction', linewidth=2)\n",
        "    axes[1].plot(history_df['epoch'], history_df['train_smoothness'], label='Smoothness', linewidth=2)\n",
        "    axes[1].set_title('Loss Components', fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. å¯è§†åŒ–åŠ¨æ€å› å­æƒé‡ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼ï¼‰\n",
        "\n",
        "å±•ç¤ºTFAå­¦åˆ°çš„æ—¶å˜å› å­é‡è¦æ€§\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºåˆ†æå™¨\n",
        "analyzer = TFAAnalyzer(tfa, device=device)\n",
        "\n",
        "# æå–å› å­æƒé‡\n",
        "weights_df = analyzer.extract_factor_weights(X_train)\n",
        "\n",
        "print(f\"\\\\nğŸ¯ Factor Weights Extracted:\")\n",
        "print(f\"  Shape: {weights_df.shape}\")\n",
        "print(f\"  Columns: {weights_df.columns.tolist()}\")\n",
        "print(f\"\\\\nSample:\")\n",
        "weights_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–å¹³å‡attention pattern\n",
        "analyzer.plot_average_attention_pattern(weights_df)\n",
        "\n",
        "print(\"\\\\nğŸ’¡ Interpretation:\")\n",
        "print(\"  - çƒ­åŠ›å›¾æ˜¾ç¤ºå“ªäº›PCAå› å­åœ¨å“ªäº›æ—¶æœŸæœ€é‡è¦\")\n",
        "print(\"  - æŠ˜çº¿å›¾æ˜¾ç¤ºattentionå¦‚ä½•éšæ—¶é—´è¡°å‡\")\n",
        "print(\"  - å¦‚æœæœ€è¿‘3ä¸ªæœˆæƒé‡é«˜ â†’ åŠ¨é‡ç­–ç•¥\")\n",
        "print(\"  - å¦‚æœå‡åŒ€åˆ†å¸ƒ â†’ é•¿æœŸå‡å€¼å›å¤ç­–ç•¥\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. åˆ†æLatent Factors\n",
        "\n",
        "æŸ¥çœ‹TFAå­¦åˆ°çš„ä½ç»´è¡¨ç¤º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æå–latent factors\n",
        "latent_df, correlations = analyzer.analyze_latent_factors(X_train, y_train)\n",
        "\n",
        "print(f\"\\\\nğŸ§¬ Latent Factors:\")\n",
        "print(f\"  Shape: {latent_df.shape}\")\n",
        "print(f\"\\\\n  Correlations with Returns:\")\n",
        "for factor, corr in correlations.items():\n",
        "    print(f\"    {factor}: {corr:.4f}\")\n",
        "\n",
        "latent_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯è§†åŒ–latent factors\n",
        "analyzer.plot_latent_factor_analysis(latent_df, correlations)\n",
        "\n",
        "print(\"\\\\nğŸ’¡ Key Observations:\")\n",
        "print(\"  1. å·¦ä¸Šï¼šå“ªä¸ªlatent factorä¸æ”¶ç›Šç›¸å…³æ€§æœ€å¼º\")\n",
        "print(\"  2. å³ä¸Šï¼šLatent factorsä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆåº”è¯¥æ¥è¿‘å¯¹è§’ï¼‰\")\n",
        "print(\"  3. å·¦ä¸‹ï¼šæœ€ä½³factorä¸æ”¶ç›Šçš„æ•£ç‚¹å›¾\")\n",
        "print(\"  4. å³ä¸‹ï¼šå„factorçš„åˆ†å¸ƒ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ç”Ÿæˆé¢„æµ‹å’Œäº¤æ˜“ä¿¡å·\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆé¢„æµ‹\n",
        "predictions = tfa.predict(X_train)\n",
        "\n",
        "print(f\"\\\\nğŸ“ˆ Predictions:\")\n",
        "print(f\"  Shape: {predictions.shape}\")\n",
        "print(f\"  Range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
        "print(f\"  Mean: {predictions.mean():.4f}\")\n",
        "\n",
        "# é¢„æµ‹vså®é™…\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# æ•£ç‚¹å›¾\n",
        "axes[0].scatter(predictions, y_train, alpha=0.3, s=10)\n",
        "axes[0].plot([predictions.min(), predictions.max()], \n",
        "             [predictions.min(), predictions.max()],\n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Predicted Return')\n",
        "axes[0].set_ylabel('Actual Return')\n",
        "axes[0].set_title('Prediction vs Actual', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# ç›¸å…³æ€§\n",
        "corr = np.corrcoef(predictions, y_train)[0, 1]\n",
        "axes[0].text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "            transform=axes[0].transAxes,\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# åˆ†æ¡£åˆ†æ\n",
        "pred_df = pd.DataFrame({'prediction': predictions, 'actual': y_train})\n",
        "pred_df['decile'] = pd.qcut(pred_df['prediction'], 10, labels=False, duplicates='drop')\n",
        "decile_returns = pred_df.groupby('decile')['actual'].mean()\n",
        "\n",
        "axes[1].bar(decile_returns.index, decile_returns.values, alpha=0.7, color='steelblue')\n",
        "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1].set_xlabel('Prediction Decile (0=Low, 9=High)')\n",
        "axes[1].set_ylabel('Average Actual Return')\n",
        "axes[1].set_title('Decile Analysis (Monotonicity Check)', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\nâœ… Good model: Decile returns should increase monotonically\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“ï¼šTFAçš„æ ¸å¿ƒä¼˜åŠ¿\n",
        "\n",
        "### ğŸ¯ åˆ›æ–°ç‚¹\n",
        "\n",
        "1. **Dynamic Factor Weighting**\n",
        "   - PCAå› å­æƒé‡ä¸æ˜¯å›ºå®šçš„\n",
        "   - æ ¹æ®å†å²36ä¸ªæœˆåŠ¨æ€è°ƒæ•´\n",
        "   - å¯è§£é‡Šï¼šçœ‹attentionå°±çŸ¥é“å“ªäº›æ—¶æœŸé‡è¦\n",
        "\n",
        "2. **Encoder-Decoderæ¶æ„**\n",
        "   - ä¸åªé¢„æµ‹ï¼Œè¿˜é‡æ„åŸå§‹PCAå› å­\n",
        "   - ç¡®ä¿å­¦åˆ°çš„è¡¨ç¤ºä¿ç•™ä¿¡æ¯\n",
        "   - é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "\n",
        "3. **Temporal Smoothness**\n",
        "   - å¹³æ»‘æ€§çº¦æŸè®©æƒé‡å˜åŒ–æ›´æ¸è¿›\n",
        "   - å¢å¼ºå¯è§£é‡Šæ€§\n",
        "   - ç¬¦åˆé‡‘èç›´è§‰\n",
        "\n",
        "### ğŸ“Š ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•\n",
        "\n",
        "| ç»´åº¦ | ä¼ ç»ŸPCA | TFA |\n",
        "|------|---------|-----|\n",
        "| å› å­æƒé‡ | é™æ€ | æ—¶å˜ |\n",
        "| æ—¶åºå»ºæ¨¡ | æ—  | Transformer |\n",
        "| å¯è§£é‡Šæ€§ | è½½è·çŸ©é˜µ | Attentionæƒé‡ |\n",
        "| é¢„æµ‹ç›®æ ‡ | æ—  | æœ‰ï¼ˆend-to-endï¼‰ |\n",
        "\n",
        "### ğŸš€ ä¸‹ä¸€æ­¥\n",
        "\n",
        "1. ç”¨å®Œæ•´æ•°æ®è®­ç»ƒï¼š`python train_tfa.py --epochs 50`\n",
        "2. å¯¹æ¯”baselineï¼šRidge, LSTMç­‰\n",
        "3. Regimeåˆ†æï¼šç‰›ç†Šå¸‚ä¸‹çš„ä¸åŒattention pattern\n",
        "4. å†™è®ºæ–‡ï¼\n",
        "\n",
        "### ğŸ“š è®ºæ–‡æ ¸å¿ƒTable\n",
        "\n",
        "```\n",
        "Table X: TFA vs Baselines\n",
        "\n",
        "Model          | IC    | ICIR  | Sharpe\n",
        "---------------|-------|-------|--------\n",
        "PCA + Ridge    | 0.042 | 0.58  | 0.95\n",
        "PCA + LSTM     | 0.052 | 0.71  | 1.28\n",
        "TFA (Ours)     | 0.065 | 0.89  | 1.65   â† ç›®æ ‡\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
